# salai/asr_finetune/speechbrain/hparams_sb.yaml
#
# SpeechBrain hyperparameters for Conformer-CTC ASR model.

# Top-level placeholders for runtime overrides
output_folder: null # This will be overridden by the training script

# Data
train_manifest: null
valid_manifest: null
test_manifest: null
sorting: "ascending" # "ascending", "descending", or "random"

# Dataloader options
dataloader_opts:
  batch_size: 1
  num_workers: 8

# Tokenizer - will be populated by the training script
tokenizer: null
output_neurons: 42 # Placeholder, will be set based on tokenizer vocab size
blank_index: 0

# Feature Extraction
sample_rate: 16000
compute_features: !new:speechbrain.lobes.features.Fbank
  n_mels: 80

# Model Architecture
# This layer projects the 80-dim FBANKs to the model's internal dimension (d_model=256)
input_linear: !new:speechbrain.nnet.linear.Linear
  input_size: 80
  n_neurons: 256

model: !new:speechbrain.lobes.models.transformer.Conformer.ConformerEncoder
  d_model: 256
  nhead: 4
  num_layers: 12
  d_ffn: 1024
  dropout: 0.1
  attention_type: "regularMHA"

ctc_lin: !new:speechbrain.nnet.linear.Linear
  input_size: 256
  n_neurons: !ref <output_neurons> # Dynamically set by tokenizer size

log_softmax: !new:speechbrain.nnet.activations.Softmax
  apply_log: True

# Loss Function
# ctc_loss is a function, not a class, so we remove the !new tag.
ctc_loss: !name:speechbrain.nnet.losses.ctc_loss
  blank_index: !ref <blank_index>

# Checkpointer
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: !ref <output_folder>/checkpoints
  recoverables:
    model: !ref <model>
    ctc_lin: !ref <ctc_lin>
    input_linear: !ref <input_linear>
    counter: !ref <epoch_counter>

# Logger
train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: !ref <output_folder>/train_log.txt

# Epoch Counter
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
  limit: 15 # This will be overridden by the --epochs flag
