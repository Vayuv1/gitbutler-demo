CMD: python /home/pandeys2/Workspace/salai/asr_finetune/atc0py_bc_whisper_finetune_input_params_sweep.py --model_name openai/whisper-medium.en --dataset_part base --train_fraction 0.25 --validation_samples 200 --epochs 1 --language en --task transcribe --per_device_train_batch_size 8 --gradient_accumulation_steps 4 --learning_rate 0.001 --lora_r 32 --lora_alpha 64 --lora_dropout 0.05 --seed 42 --num_proc 1 --eval_strategy epoch --eval_steps 100 --run_stage coarse --run_id coarse_004 --skip_save --metrics_csv /home/pandeys2/Workspace/salai/asr_finetune/sweep_runs/tmp_metrics/coarse_004.csv
Using the latest cached version of the module from /home/pandeys2/.cache/huggingface/modules/datasets_modules/datasets/HF-SaLAI--salai_atc0/6a6a809cccd83aaa7dae50203e4c7f7fc6313a701a3deebe3dfca35a211f125c (last modified on Wed Aug  6 19:14:44 2025) since it couldn't be found locally at HF-SaLAI/salai_atc0, or remotely on the Hugging Face Hub.
/home/pandeys2/Workspace/salai/asr_finetune/atc0py_bc_whisper_finetune_input_params_sweep.py:326: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
Using device: cuda
---- PATH DEBUG ----
CWD:            /home/pandeys2/Workspace/salai
__file__:       /home/pandeys2/Workspace/salai/asr_finetune/atc0py_bc_whisper_finetune_input_params_sweep.py
script_dir:     /home/pandeys2/Workspace/salai/asr_finetune
repo_root:      /home/pandeys2/Workspace/salai
finetuned_dir:  /home/pandeys2/Workspace/salai/finetuned_models
OUTPUT DIR:     /home/pandeys2/Workspace/salai/finetuned_models/whisper_lora_whisper-medium.en_base
--------------------
trainable params: 9,437,184 || all params: 773,294,080 || trainable%: 1.2204
  0%|          | 0/54 [00:00<?, ?it/s]/home/pandeys2/Workspace/salai/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  2%|▏         | 1/54 [00:04<03:45,  4.26s/it]  4%|▎         | 2/54 [00:07<03:22,  3.89s/it]  6%|▌         | 3/54 [00:11<03:12,  3.77s/it]  7%|▋         | 4/54 [00:15<03:06,  3.73s/it]  9%|▉         | 5/54 [00:18<03:00,  3.69s/it] 11%|█         | 6/54 [00:22<02:56,  3.68s/it] 13%|█▎        | 7/54 [00:26<02:51,  3.66s/it] 15%|█▍        | 8/54 [00:29<02:48,  3.65s/it] 17%|█▋        | 9/54 [00:33<02:43,  3.64s/it] 19%|█▊        | 10/54 [00:36<02:39,  3.63s/it]                                                19%|█▊        | 10/54 [00:36<02:39,  3.63s/it] 20%|██        | 11/54 [00:40<02:36,  3.63s/it] 22%|██▏       | 12/54 [00:44<02:32,  3.64s/it] 24%|██▍       | 13/54 [00:47<02:28,  3.63s/it] 26%|██▌       | 14/54 [00:51<02:24,  3.62s/it] 28%|██▊       | 15/54 [00:55<02:21,  3.63s/it] 30%|██▉       | 16/54 [00:58<02:17,  3.63s/it] 31%|███▏      | 17/54 [01:02<02:14,  3.63s/it] 33%|███▎      | 18/54 [01:05<02:10,  3.63s/it] 35%|███▌      | 19/54 [01:09<02:06,  3.63s/it] 37%|███▋      | 20/54 [01:13<02:03,  3.63s/it]                                                37%|███▋      | 20/54 [01:13<02:03,  3.63s/it] 39%|███▉      | 21/54 [01:16<02:00,  3.64s/it] 41%|████      | 22/54 [01:20<01:56,  3.63s/it] 43%|████▎     | 23/54 [01:24<01:52,  3.63s/it] 44%|████▍     | 24/54 [01:27<01:48,  3.62s/it] 46%|████▋     | 25/54 [01:31<01:45,  3.64s/it] 48%|████▊     | 26/54 [01:35<01:41,  3.64s/it] 50%|█████     | 27/54 [01:38<01:38,  3.63s/it] 52%|█████▏    | 28/54 [01:42<01:34,  3.64s/it] 54%|█████▎    | 29/54 [01:45<01:30,  3.63s/it] 56%|█████▌    | 30/54 [01:49<01:27,  3.63s/it]                                                56%|█████▌    | 30/54 [01:49<01:27,  3.63s/it] 57%|█████▋    | 31/54 [01:53<01:23,  3.62s/it] 59%|█████▉    | 32/54 [01:56<01:20,  3.64s/it] 61%|██████    | 33/54 [02:00<01:16,  3.63s/it] 63%|██████▎   | 34/54 [02:04<01:12,  3.63s/it] 65%|██████▍   | 35/54 [02:07<01:08,  3.62s/it] 67%|██████▋   | 36/54 [02:11<01:05,  3.62s/it] 69%|██████▊   | 37/54 [02:14<01:01,  3.62s/it] 70%|███████   | 38/54 [02:18<00:57,  3.62s/it] 72%|███████▏  | 39/54 [02:22<00:54,  3.63s/it] 74%|███████▍  | 40/54 [02:25<00:50,  3.63s/it]                                                74%|███████▍  | 40/54 [02:25<00:50,  3.63s/it] 76%|███████▌  | 41/54 [02:29<00:47,  3.63s/it] 78%|███████▊  | 42/54 [02:33<00:43,  3.63s/it] 80%|███████▉  | 43/54 [02:36<00:39,  3.63s/it] 81%|████████▏ | 44/54 [02:40<00:36,  3.63s/it] 83%|████████▎ | 45/54 [02:44<00:32,  3.65s/it] 85%|████████▌ | 46/54 [02:47<00:29,  3.63s/it] 87%|████████▋ | 47/54 [02:51<00:25,  3.63s/it] 89%|████████▉ | 48/54 [02:54<00:21,  3.62s/it] 91%|█████████ | 49/54 [02:58<00:18,  3.62s/it] 93%|█████████▎| 50/54 [03:02<00:14,  3.63s/it]                                                93%|█████████▎| 50/54 [03:02<00:14,  3.63s/it] 94%|█████████▍| 51/54 [03:05<00:10,  3.66s/it] 96%|█████████▋| 52/54 [03:09<00:07,  3.65s/it] 98%|█████████▊| 53/54 [03:13<00:03,  3.63s/it]100%|██████████| 54/54 [03:13<00:00,  2.67s/it]Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
{'loss': 2.4736, 'grad_norm': 1.8944789171218872, 'learning_rate': 0.0008518518518518519, 'epoch': 0.19}
{'loss': 0.9161, 'grad_norm': 1.0194391012191772, 'learning_rate': 0.0006666666666666666, 'epoch': 0.38}
{'loss': 0.8005, 'grad_norm': 1.0763089656829834, 'learning_rate': 0.00048148148148148144, 'epoch': 0.56}
{'loss': 0.6761, 'grad_norm': 1.9058791399002075, 'learning_rate': 0.0002962962962962963, 'epoch': 0.75}
{'loss': 0.5886, 'grad_norm': 0.9107329249382019, 'learning_rate': 0.0001111111111111111, 'epoch': 0.94}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:01<00:18,  1.24it/s][A
 12%|█▏        | 3/25 [00:03<00:23,  1.05s/it][A
 16%|█▌        | 4/25 [00:04<00:27,  1.31s/it][A
 20%|██        | 5/25 [00:06<00:28,  1.43s/it][A
 24%|██▍       | 6/25 [00:08<00:32,  1.70s/it][A
 28%|██▊       | 7/25 [00:10<00:29,  1.65s/it][A
 32%|███▏      | 8/25 [00:11<00:26,  1.56s/it][A
 36%|███▌      | 9/25 [00:13<00:25,  1.59s/it][A
 40%|████      | 10/25 [00:14<00:22,  1.50s/it][A
 44%|████▍     | 11/25 [00:16<00:24,  1.72s/it][A
 48%|████▊     | 12/25 [00:18<00:21,  1.67s/it][A
 52%|█████▏    | 13/25 [00:19<00:19,  1.65s/it][A
 56%|█████▌    | 14/25 [00:21<00:17,  1.63s/it][A
 60%|██████    | 15/25 [00:23<00:17,  1.73s/it][A
 64%|██████▍   | 16/25 [00:26<00:18,  2.01s/it][A
 68%|██████▊   | 17/25 [00:27<00:15,  1.91s/it][A
 72%|███████▏  | 18/25 [00:29<00:13,  1.93s/it][A
 76%|███████▌  | 19/25 [00:31<00:11,  1.92s/it][A
 80%|████████  | 20/25 [00:33<00:08,  1.76s/it][A
 84%|████████▍ | 21/25 [00:35<00:08,  2.05s/it][A
 88%|████████▊ | 22/25 [00:37<00:06,  2.11s/it][A
 92%|█████████▏| 23/25 [00:39<00:03,  1.94s/it][A
 96%|█████████▌| 24/25 [00:40<00:01,  1.78s/it][A
100%|██████████| 25/25 [00:42<00:00,  1.63s/it][A                                               
                                               [A100%|██████████| 54/54 [04:01<00:00,  2.67s/it]
100%|██████████| 25/25 [00:45<00:00,  1.63s/it][A
                                               [A                                               100%|██████████| 54/54 [04:01<00:00,  2.67s/it]100%|██████████| 54/54 [04:01<00:00,  4.48s/it]
{'eval_loss': 0.562350869178772, 'eval_wer': 21.497998856489424, 'eval_runtime': 48.305, 'eval_samples_per_second': 4.12, 'eval_steps_per_second': 0.518, 'epoch': 1.0}
{'train_runtime': 241.7753, 'train_samples_per_second': 7.04, 'train_steps_per_second': 0.223, 'train_loss': 1.0450077012733177, 'epoch': 1.0}
Training complete.
Skipping all weight saves (--skip_save).
  0%|          | 0/25 [00:00<?, ?it/s]  8%|▊         | 2/25 [00:01<00:18,  1.24it/s] 12%|█▏        | 3/25 [00:03<00:23,  1.05s/it] 16%|█▌        | 4/25 [00:04<00:27,  1.31s/it] 20%|██        | 5/25 [00:06<00:28,  1.43s/it] 24%|██▍       | 6/25 [00:08<00:32,  1.72s/it] 28%|██▊       | 7/25 [00:10<00:29,  1.66s/it] 32%|███▏      | 8/25 [00:11<00:26,  1.57s/it] 36%|███▌      | 9/25 [00:13<00:25,  1.59s/it] 40%|████      | 10/25 [00:14<00:22,  1.51s/it] 44%|████▍     | 11/25 [00:16<00:24,  1.72s/it] 48%|████▊     | 12/25 [00:18<00:21,  1.66s/it] 52%|█████▏    | 13/25 [00:19<00:19,  1.64s/it] 56%|█████▌    | 14/25 [00:21<00:17,  1.62s/it] 60%|██████    | 15/25 [00:23<00:17,  1.73s/it] 64%|██████▍   | 16/25 [00:26<00:17,  1.99s/it] 68%|██████▊   | 17/25 [00:27<00:15,  1.91s/it] 72%|███████▏  | 18/25 [00:29<00:13,  1.93s/it] 76%|███████▌  | 19/25 [00:31<00:11,  1.91s/it] 80%|████████  | 20/25 [00:33<00:08,  1.75s/it] 84%|████████▍ | 21/25 [00:35<00:08,  2.04s/it] 88%|████████▊ | 22/25 [00:37<00:06,  2.10s/it] 92%|█████████▏| 23/25 [00:39<00:03,  1.95s/it] 96%|█████████▌| 24/25 [00:40<00:01,  1.79s/it]100%|██████████| 25/25 [00:42<00:00,  1.63s/it]100%|██████████| 25/25 [00:45<00:00,  1.83s/it]
[sweep] Metrics written to: /home/pandeys2/Workspace/salai/asr_finetune/sweep_runs/tmp_metrics/coarse_004.csv
