CMD: python /home/pandeys2/Workspace/salai/asr_finetune/atc0py_bc_whisper_finetune_input_params_sweep.py --model_name openai/whisper-large-v3-turbo --dataset_part base --train_fraction 0.25 --validation_samples 200 --epochs 1 --language en --task transcribe --per_device_train_batch_size 6 --gradient_accumulation_steps 6 --learning_rate 0.001 --lora_r 32 --lora_alpha 64 --lora_dropout 0.05 --seed 42 --num_proc 1 --eval_strategy epoch --eval_steps 100 --run_stage coarse --run_id coarse_028 --skip_save --metrics_csv /home/pandeys2/Workspace/salai/asr_finetune/sweep_runs/tmp_metrics/coarse_028.csv
Using the latest cached version of the module from /home/pandeys2/.cache/huggingface/modules/datasets_modules/datasets/HF-SaLAI--salai_atc0/6a6a809cccd83aaa7dae50203e4c7f7fc6313a701a3deebe3dfca35a211f125c (last modified on Wed Aug  6 19:14:44 2025) since it couldn't be found locally at HF-SaLAI/salai_atc0, or remotely on the Hugging Face Hub.
/home/pandeys2/Workspace/salai/asr_finetune/atc0py_bc_whisper_finetune_input_params_sweep.py:326: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
Using device: cuda
---- PATH DEBUG ----
CWD:            /home/pandeys2/Workspace/salai
__file__:       /home/pandeys2/Workspace/salai/asr_finetune/atc0py_bc_whisper_finetune_input_params_sweep.py
script_dir:     /home/pandeys2/Workspace/salai/asr_finetune
repo_root:      /home/pandeys2/Workspace/salai
finetuned_dir:  /home/pandeys2/Workspace/salai/finetuned_models
OUTPUT DIR:     /home/pandeys2/Workspace/salai/finetuned_models/whisper_lora_whisper-large-v3-turbo_base
--------------------
trainable params: 6,553,600 || all params: 815,431,680 || trainable%: 0.8037
  0%|          | 0/48 [00:00<?, ?it/s]/home/pandeys2/Workspace/salai/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  2%|▏         | 1/48 [00:06<05:15,  6.71s/it]  4%|▍         | 2/48 [00:12<04:50,  6.31s/it]  6%|▋         | 3/48 [00:18<04:38,  6.20s/it]  8%|▊         | 4/48 [00:24<04:30,  6.14s/it] 10%|█         | 5/48 [00:30<04:22,  6.10s/it] 12%|█▎        | 6/48 [00:36<04:15,  6.08s/it] 15%|█▍        | 7/48 [00:42<04:08,  6.07s/it] 17%|█▋        | 8/48 [00:48<04:02,  6.05s/it] 19%|█▉        | 9/48 [00:55<03:56,  6.06s/it] 21%|██        | 10/48 [01:01<03:49,  6.05s/it]                                                21%|██        | 10/48 [01:01<03:49,  6.05s/it] 23%|██▎       | 11/48 [01:07<03:43,  6.04s/it] 25%|██▌       | 12/48 [01:13<03:37,  6.04s/it] 27%|██▋       | 13/48 [01:19<03:31,  6.04s/it] 29%|██▉       | 14/48 [01:25<03:25,  6.04s/it] 31%|███▏      | 15/48 [01:31<03:19,  6.04s/it] 33%|███▎      | 16/48 [01:37<03:13,  6.03s/it] 35%|███▌      | 17/48 [01:43<03:07,  6.04s/it] 38%|███▊      | 18/48 [01:49<03:00,  6.03s/it] 40%|███▉      | 19/48 [01:55<02:55,  6.05s/it] 42%|████▏     | 20/48 [02:01<02:49,  6.07s/it]                                                42%|████▏     | 20/48 [02:01<02:49,  6.07s/it] 44%|████▍     | 21/48 [02:07<02:43,  6.06s/it] 46%|████▌     | 22/48 [02:13<02:37,  6.07s/it] 48%|████▊     | 23/48 [02:19<02:31,  6.06s/it] 50%|█████     | 24/48 [02:25<02:25,  6.07s/it] 52%|█████▏    | 25/48 [02:31<02:19,  6.06s/it] 54%|█████▍    | 26/48 [02:37<02:13,  6.06s/it] 56%|█████▋    | 27/48 [02:43<02:07,  6.06s/it] 58%|█████▊    | 28/48 [02:50<02:00,  6.05s/it] 60%|██████    | 29/48 [02:56<01:55,  6.07s/it] 62%|██████▎   | 30/48 [03:02<01:49,  6.07s/it]                                                62%|██████▎   | 30/48 [03:02<01:49,  6.07s/it] 65%|██████▍   | 31/48 [03:08<01:42,  6.06s/it] 67%|██████▋   | 32/48 [03:14<01:36,  6.04s/it] 69%|██████▉   | 33/48 [03:20<01:30,  6.04s/it] 71%|███████   | 34/48 [03:26<01:24,  6.04s/it] 73%|███████▎  | 35/48 [03:32<01:18,  6.03s/it] 75%|███████▌  | 36/48 [03:38<01:12,  6.03s/it] 77%|███████▋  | 37/48 [03:44<01:06,  6.05s/it] 79%|███████▉  | 38/48 [03:50<01:00,  6.04s/it] 81%|████████▏ | 39/48 [03:56<00:54,  6.03s/it] 83%|████████▎ | 40/48 [04:02<00:48,  6.03s/it]                                                83%|████████▎ | 40/48 [04:02<00:48,  6.03s/it] 85%|████████▌ | 41/48 [04:08<00:42,  6.03s/it] 88%|████████▊ | 42/48 [04:14<00:36,  6.05s/it] 90%|████████▉ | 43/48 [04:20<00:30,  6.04s/it] 92%|█████████▏| 44/48 [04:26<00:24,  6.04s/it] 94%|█████████▍| 45/48 [04:32<00:18,  6.03s/it] 96%|█████████▌| 46/48 [04:38<00:12,  6.03s/it] 98%|█████████▊| 47/48 [04:44<00:06,  6.04s/it]100%|██████████| 48/48 [04:46<00:00,  4.60s/it]Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.
Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
{'loss': 1.9089, 'grad_norm': 2.0481116771698, 'learning_rate': 0.0008125000000000001, 'epoch': 0.21}
{'loss': 0.7832, 'grad_norm': 1.670130729675293, 'learning_rate': 0.0006041666666666666, 'epoch': 0.42}
{'loss': 0.6301, 'grad_norm': 1.7289702892303467, 'learning_rate': 0.0003958333333333333, 'epoch': 0.63}
{'loss': 0.5858, 'grad_norm': 1.3115241527557373, 'learning_rate': 0.00020833333333333335, 'epoch': 0.85}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:01<00:17,  1.35it/s][A
 12%|█▏        | 3/25 [00:02<00:22,  1.03s/it][A
 16%|█▌        | 4/25 [00:04<00:25,  1.20s/it][A
 20%|██        | 5/25 [00:05<00:25,  1.29s/it][A
 24%|██▍       | 6/25 [00:07<00:26,  1.38s/it][A
 28%|██▊       | 7/25 [00:08<00:25,  1.41s/it][A
 32%|███▏      | 8/25 [00:10<00:24,  1.42s/it][A
 36%|███▌      | 9/25 [00:11<00:22,  1.43s/it][A
 40%|████      | 10/25 [00:13<00:21,  1.43s/it][A
 44%|████▍     | 11/25 [00:14<00:20,  1.47s/it][A
 48%|████▊     | 12/25 [00:16<00:19,  1.47s/it][A
 52%|█████▏    | 13/25 [00:17<00:17,  1.47s/it][A
 56%|█████▌    | 14/25 [00:19<00:16,  1.46s/it][A
 60%|██████    | 15/25 [00:20<00:14,  1.48s/it][A
 64%|██████▍   | 16/25 [00:22<00:13,  1.53s/it][A
 68%|██████▊   | 17/25 [00:23<00:12,  1.51s/it][A
 72%|███████▏  | 18/25 [00:25<00:10,  1.51s/it][A
 76%|███████▌  | 19/25 [00:26<00:09,  1.51s/it][A
 80%|████████  | 20/25 [00:28<00:07,  1.48s/it][A
 84%|████████▍ | 21/25 [00:29<00:06,  1.54s/it][A
 88%|████████▊ | 22/25 [00:31<00:04,  1.55s/it][A
 92%|█████████▏| 23/25 [00:32<00:03,  1.53s/it][A
 96%|█████████▌| 24/25 [00:34<00:01,  1.48s/it][A
100%|██████████| 25/25 [00:35<00:00,  1.29s/it][A                                               
                                               [A100%|██████████| 48/48 [05:26<00:00,  4.60s/it]
100%|██████████| 25/25 [00:38<00:00,  1.29s/it][A
                                               [A                                               100%|██████████| 48/48 [05:26<00:00,  4.60s/it]100%|██████████| 48/48 [05:26<00:00,  6.81s/it]
{'eval_loss': 0.49815645813941956, 'eval_wer': 20.640365923384792, 'eval_runtime': 40.8117, 'eval_samples_per_second': 4.876, 'eval_steps_per_second': 0.613, 'epoch': 1.0}
{'train_runtime': 326.8463, 'train_samples_per_second': 5.207, 'train_steps_per_second': 0.147, 'train_loss': 0.8956926961739858, 'epoch': 1.0}
Training complete.
Skipping all weight saves (--skip_save).
  0%|          | 0/25 [00:00<?, ?it/s]  8%|▊         | 2/25 [00:01<00:17,  1.35it/s] 12%|█▏        | 3/25 [00:02<00:22,  1.03s/it] 16%|█▌        | 4/25 [00:04<00:25,  1.20s/it] 20%|██        | 5/25 [00:05<00:25,  1.29s/it] 24%|██▍       | 6/25 [00:07<00:26,  1.38s/it] 28%|██▊       | 7/25 [00:08<00:25,  1.40s/it] 32%|███▏      | 8/25 [00:10<00:24,  1.42s/it] 36%|███▌      | 9/25 [00:11<00:23,  1.45s/it] 40%|████      | 10/25 [00:13<00:21,  1.45s/it] 44%|████▍     | 11/25 [00:14<00:20,  1.48s/it] 48%|████▊     | 12/25 [00:16<00:19,  1.49s/it] 52%|█████▏    | 13/25 [00:17<00:17,  1.48s/it] 56%|█████▌    | 14/25 [00:19<00:16,  1.47s/it] 60%|██████    | 15/25 [00:20<00:14,  1.49s/it] 64%|██████▍   | 16/25 [00:22<00:13,  1.55s/it] 68%|██████▊   | 17/25 [00:23<00:12,  1.53s/it] 72%|███████▏  | 18/25 [00:25<00:10,  1.54s/it] 76%|███████▌  | 19/25 [00:27<00:09,  1.54s/it] 80%|████████  | 20/25 [00:28<00:07,  1.52s/it] 84%|████████▍ | 21/25 [00:30<00:06,  1.55s/it] 88%|████████▊ | 22/25 [00:31<00:04,  1.56s/it] 92%|█████████▏| 23/25 [00:33<00:03,  1.53s/it] 96%|█████████▌| 24/25 [00:34<00:01,  1.49s/it]100%|██████████| 25/25 [00:35<00:00,  1.29s/it]100%|██████████| 25/25 [00:38<00:00,  1.56s/it]
[sweep] Metrics written to: /home/pandeys2/Workspace/salai/asr_finetune/sweep_runs/tmp_metrics/coarse_028.csv
