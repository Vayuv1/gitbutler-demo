CMD: python /home/pandeys2/Workspace/salai/asr_finetune/atc0py_bc_whisper_finetune_input_params_sweep.py --model_name openai/whisper-large-v3-turbo --dataset_part base --train_fraction 0.25 --validation_samples 200 --epochs 1 --language en --task transcribe --per_device_train_batch_size 6 --gradient_accumulation_steps 6 --learning_rate 0.0005 --lora_r 16 --lora_alpha 32 --lora_dropout 0.05 --seed 42 --num_proc 1 --eval_strategy epoch --eval_steps 100 --run_stage coarse --run_id coarse_025 --skip_save --metrics_csv /home/pandeys2/Workspace/salai/asr_finetune/sweep_runs/tmp_metrics/coarse_025.csv
Using the latest cached version of the module from /home/pandeys2/.cache/huggingface/modules/datasets_modules/datasets/HF-SaLAI--salai_atc0/6a6a809cccd83aaa7dae50203e4c7f7fc6313a701a3deebe3dfca35a211f125c (last modified on Wed Aug  6 19:14:44 2025) since it couldn't be found locally at HF-SaLAI/salai_atc0, or remotely on the Hugging Face Hub.
Using device: cuda
---- PATH DEBUG ----
CWD:            /home/pandeys2/Workspace/salai
__file__:       /home/pandeys2/Workspace/salai/asr_finetune/atc0py_bc_whisper_finetune_input_params_sweep.py
script_dir:     /home/pandeys2/Workspace/salai/asr_finetune
repo_root:      /home/pandeys2/Workspace/salai
finetuned_dir:  /home/pandeys2/Workspace/salai/finetuned_models
OUTPUT DIR:     /home/pandeys2/Workspace/salai/finetuned_models/whisper_lora_whisper-large-v3-turbo_base
--------------------
Filter:   0%|          | 0/6853 [00:00<?, ? examples/s]/home/pandeys2/Workspace/salai/.venv/lib/python3.12/site-packages/transformers/models/whisper/tokenization_whisper.py:503: UserWarning: The private method `_normalize` is deprecated and will be removed in v5 of Transformers.You can normalize an input string using the Whisper English normalizer using the `normalize` method.
  warnings.warn(
Filter:  15%|â–ˆâ–        | 1000/6853 [00:00<00:03, 1464.29 examples/s]Filter:  29%|â–ˆâ–ˆâ–‰       | 2000/6853 [00:00<00:02, 2215.91 examples/s]Filter:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 3000/6853 [00:01<00:01, 2586.89 examples/s]Filter:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 4000/6853 [00:01<00:01, 2808.08 examples/s]Filter:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 5000/6853 [00:01<00:00, 3026.66 examples/s]Filter:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 6000/6853 [00:02<00:00, 3237.60 examples/s]Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6853/6853 [00:02<00:00, 3213.17 examples/s]Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6853/6853 [00:02<00:00, 2838.91 examples/s]
Filter:   0%|          | 0/200 [00:00<?, ? examples/s]Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 4193.74 examples/s]
Map:   0%|          | 0/1702 [00:00<?, ? examples/s]Map:   3%|â–Ž         | 43/1702 [00:00<00:04, 413.54 examples/s]Map:   5%|â–         | 85/1702 [00:00<00:03, 409.48 examples/s]Map:   8%|â–Š         | 130/1702 [00:00<00:03, 422.61 examples/s]Map:  10%|â–ˆ         | 176/1702 [00:00<00:03, 434.95 examples/s]Map:  13%|â–ˆâ–Ž        | 221/1702 [00:00<00:03, 436.56 examples/s]Map:  16%|â–ˆâ–Œ        | 266/1702 [00:00<00:03, 435.63 examples/s]Map:  18%|â–ˆâ–Š        | 311/1702 [00:00<00:03, 438.43 examples/s]Map:  21%|â–ˆâ–ˆ        | 357/1702 [00:00<00:03, 443.59 examples/s]Map:  24%|â–ˆâ–ˆâ–       | 405/1702 [00:00<00:02, 451.47 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 452/1702 [00:01<00:02, 455.12 examples/s]Map:  29%|â–ˆâ–ˆâ–‰       | 498/1702 [00:01<00:02, 456.15 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 567/1702 [00:01<00:02, 454.28 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 614/1702 [00:01<00:02, 455.58 examples/s]Map:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 661/1702 [00:01<00:02, 456.25 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 730/1702 [00:01<00:02, 452.13 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 778/1702 [00:01<00:02, 457.13 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 826/1702 [00:01<00:01, 460.48 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 893/1702 [00:01<00:01, 453.96 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 961/1702 [00:02<00:01, 451.36 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1025/1702 [00:03<00:06, 103.83 examples/s]Map:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1075/1702 [00:03<00:04, 130.56 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1124/1702 [00:03<00:03, 162.17 examples/s]Map:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1173/1702 [00:04<00:02, 198.55 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1223/1702 [00:04<00:01, 239.72 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1272/1702 [00:04<00:01, 280.08 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1322/1702 [00:04<00:01, 320.27 examples/s]Map:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1372/1702 [00:04<00:00, 357.09 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1422/1702 [00:04<00:00, 389.01 examples/s]Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1471/1702 [00:04<00:00, 411.58 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1521/1702 [00:04<00:00, 431.17 examples/s]Map:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1571/1702 [00:04<00:00, 448.59 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1621/1702 [00:05<00:00, 460.45 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1670/1702 [00:05<00:00, 466.64 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1702/1702 [00:07<00:00, 242.39 examples/s]
Map:   0%|          | 0/199 [00:00<?, ? examples/s]Map:  24%|â–ˆâ–ˆâ–       | 48/199 [00:00<00:00, 465.42 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/199 [00:00<00:00, 479.61 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 147/199 [00:00<00:00, 476.36 examples/s]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 197/199 [00:00<00:00, 482.30 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199/199 [00:00<00:00, 277.92 examples/s]
/home/pandeys2/Workspace/salai/asr_finetune/atc0py_bc_whisper_finetune_input_params_sweep.py:326: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
trainable params: 3,276,800 || all params: 812,154,880 || trainable%: 0.4035
  0%|          | 0/48 [00:00<?, ?it/s]/home/pandeys2/Workspace/salai/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  2%|â–         | 1/48 [00:06<05:12,  6.65s/it]  4%|â–         | 2/48 [00:12<04:48,  6.28s/it]  6%|â–‹         | 3/48 [00:18<04:37,  6.17s/it]  8%|â–Š         | 4/48 [00:24<04:29,  6.12s/it] 10%|â–ˆ         | 5/48 [00:30<04:21,  6.08s/it] 12%|â–ˆâ–Ž        | 6/48 [00:36<04:15,  6.08s/it] 15%|â–ˆâ–        | 7/48 [00:42<04:09,  6.09s/it] 17%|â–ˆâ–‹        | 8/48 [00:48<04:02,  6.07s/it] 19%|â–ˆâ–‰        | 9/48 [00:55<03:56,  6.06s/it] 21%|â–ˆâ–ˆ        | 10/48 [01:01<03:50,  6.07s/it]                                                21%|â–ˆâ–ˆ        | 10/48 [01:01<03:50,  6.07s/it] 23%|â–ˆâ–ˆâ–Ž       | 11/48 [01:07<03:44,  6.06s/it] 25%|â–ˆâ–ˆâ–Œ       | 12/48 [01:13<03:38,  6.06s/it] 27%|â–ˆâ–ˆâ–‹       | 13/48 [01:19<03:31,  6.05s/it] 29%|â–ˆâ–ˆâ–‰       | 14/48 [01:25<03:25,  6.04s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 15/48 [01:31<03:19,  6.04s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 16/48 [01:37<03:13,  6.04s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 17/48 [01:43<03:06,  6.03s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 18/48 [01:49<03:00,  6.02s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 19/48 [01:55<02:54,  6.01s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 20/48 [02:01<02:48,  6.02s/it]                                                42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 20/48 [02:01<02:48,  6.02s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/48 [02:07<02:42,  6.02s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 22/48 [02:13<02:36,  6.02s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 23/48 [02:19<02:30,  6.02s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 24/48 [02:25<02:24,  6.03s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 25/48 [02:31<02:18,  6.02s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/48 [02:37<02:12,  6.01s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 27/48 [02:43<02:06,  6.01s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 28/48 [02:49<02:00,  6.01s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 29/48 [02:55<01:54,  6.00s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 30/48 [03:01<01:48,  6.00s/it]                                                62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 30/48 [03:01<01:48,  6.00s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/48 [03:07<01:42,  6.00s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 32/48 [03:13<01:36,  6.01s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 33/48 [03:19<01:30,  6.01s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 34/48 [03:25<01:24,  6.04s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 35/48 [03:31<01:18,  6.03s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 36/48 [03:37<01:12,  6.03s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 37/48 [03:43<01:06,  6.02s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 38/48 [03:49<01:00,  6.02s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 39/48 [03:55<00:54,  6.01s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 40/48 [04:01<00:48,  6.01s/it]                                                83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 40/48 [04:01<00:48,  6.01s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 41/48 [04:07<00:42,  6.00s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 42/48 [04:13<00:36,  6.00s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 43/48 [04:19<00:30,  6.00s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 44/48 [04:25<00:24,  6.01s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45/48 [04:31<00:18,  6.03s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 46/48 [04:37<00:12,  6.02s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 47/48 [04:43<00:06,  6.02s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [04:45<00:00,  4.59s/it]Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.
Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
{'loss': 2.5764, 'grad_norm': 2.490300178527832, 'learning_rate': 0.00040625000000000004, 'epoch': 0.21}
{'loss': 1.2089, 'grad_norm': 2.2163994312286377, 'learning_rate': 0.0003020833333333333, 'epoch': 0.42}
{'loss': 0.8374, 'grad_norm': 1.8416004180908203, 'learning_rate': 0.00020833333333333335, 'epoch': 0.63}
{'loss': 0.7332, 'grad_norm': 1.6022682189941406, 'learning_rate': 0.00010416666666666667, 'epoch': 0.85}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|â–Š         | 2/25 [00:01<00:16,  1.36it/s][A
 12%|â–ˆâ–        | 3/25 [00:02<00:22,  1.03s/it][A
 16%|â–ˆâ–Œ        | 4/25 [00:04<00:25,  1.19s/it][A
 20%|â–ˆâ–ˆ        | 5/25 [00:05<00:26,  1.30s/it][A
 24%|â–ˆâ–ˆâ–       | 6/25 [00:07<00:26,  1.38s/it][A
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:08<00:25,  1.41s/it][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:10<00:24,  1.41s/it][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:11<00:22,  1.43s/it][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:13<00:21,  1.43s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:14<00:20,  1.47s/it][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:16<00:19,  1.46s/it][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:17<00:17,  1.47s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:19<00:16,  1.46s/it][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:20<00:14,  1.48s/it][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:22<00:13,  1.53s/it][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:23<00:12,  1.52s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:25<00:10,  1.53s/it][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:26<00:09,  1.53s/it][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:28<00:07,  1.50s/it][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:29<00:06,  1.54s/it][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:31<00:04,  1.55s/it][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:33<00:03,  1.52s/it][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:34<00:01,  1.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:35<00:00,  1.28s/it][A                                               
                                               [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [05:25<00:00,  4.59s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:38<00:00,  1.28s/it][A
                                               [A                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [05:25<00:00,  4.59s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [05:25<00:00,  6.79s/it]
{'eval_loss': 0.6442083716392517, 'eval_wer': 26.815323041738136, 'eval_runtime': 40.7422, 'eval_samples_per_second': 4.884, 'eval_steps_per_second': 0.614, 'epoch': 1.0}
{'train_runtime': 325.7672, 'train_samples_per_second': 5.225, 'train_steps_per_second': 0.147, 'train_loss': 1.2251192430655162, 'epoch': 1.0}
Training complete.
Skipping all weight saves (--skip_save).
  0%|          | 0/25 [00:00<?, ?it/s]  8%|â–Š         | 2/25 [00:01<00:16,  1.37it/s] 12%|â–ˆâ–        | 3/25 [00:02<00:22,  1.02s/it] 16%|â–ˆâ–Œ        | 4/25 [00:04<00:24,  1.19s/it] 20%|â–ˆâ–ˆ        | 5/25 [00:05<00:25,  1.28s/it] 24%|â–ˆâ–ˆâ–       | 6/25 [00:07<00:26,  1.37s/it] 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:08<00:25,  1.40s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:10<00:24,  1.41s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:11<00:22,  1.43s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:13<00:21,  1.42s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:14<00:20,  1.47s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:16<00:18,  1.46s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:17<00:17,  1.46s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:19<00:16,  1.47s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:20<00:14,  1.48s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:22<00:13,  1.53s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:23<00:12,  1.53s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:25<00:10,  1.52s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:26<00:09,  1.52s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:28<00:07,  1.49s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:29<00:06,  1.55s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:31<00:04,  1.55s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:32<00:03,  1.52s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:34<00:01,  1.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:35<00:00,  1.28s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:38<00:00,  1.54s/it]
[sweep] Metrics written to: /home/pandeys2/Workspace/salai/asr_finetune/sweep_runs/tmp_metrics/coarse_025.csv
